{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "confirmed-provincial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.1.0\n",
      "Pandas version: 1.1.5\n",
      "Keras version: 2.2.4-tf\n",
      "Pillow version: 8.1.0 \n",
      "\n",
      "GPU's: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Import tensorflow.\n",
    "import tensorflow as tf\n",
    "\n",
    "# PIL (Pillow) for working with images.\n",
    "import PIL\n",
    "from PIL import Image , ImageOps\n",
    "\n",
    "# Used for globbing up directories.\n",
    "import glob\n",
    "\n",
    "# The Data Tools \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the Keras tools.\n",
    "from tensorflow import keras\n",
    "\n",
    "# Network type.\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Layer information.\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout #This right here is the convolutional NN\n",
    "\n",
    "# For editing images. \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator #image processor that keras has built in\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "print(\"Pandas version:\", pd.__version__)\n",
    "print(\"Keras version:\", keras.__version__)\n",
    "print(\"Pillow version:\",PIL.__version__,\"\\n\")\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "print(\"GPU's:\",physical_devices)\n",
    "for gpu_instance in physical_devices: \n",
    "    tf.config.experimental.set_memory_growth(gpu_instance, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "compact-deposit",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path_train = \"../Data/recreation/10Knots/Figure-8 Loop/DiffuseLight\" #good data handling, always keep data in same place so can always find the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "governmental-conducting",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator( #This is creating a training data GENERATOR, stnd up object with our settings and tell keras that each image you see, apply these things to it.\n",
    "    #Maybe you have keras look at 5 or 10 images that it will use to train, instead of hundreds; trying to design this so we get enough resolution to capture gesture\n",
    "    #By changing the below values, you get different resolutuion, these numbers are a range   \n",
    "    rescale=1 / 255.0,\n",
    "        rotation_range=20,\n",
    "        zoom_range=0.05,\n",
    "        width_shift_range=0.05,\n",
    "        height_shift_range=0.05,\n",
    "        shear_range=0.05,\n",
    "        horizontal_flip=True, #be able to see it upside down\n",
    "        fill_mode=\"nearest\",\n",
    "        validation_split=0.20) # 20% validation data set\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1 / 255.0)\n",
    "#This above is changing the images so we can try and train on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "literary-stocks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8 #Batch size is going to limited by GPUs for our project. \n",
    "train_generator = train_datagen.flow_from_directory( #This is the generator; flow from directory means that it takes directly from directory, which allows for RAM savings.\n",
    "    #Keras will only take what it needs, and data is already categorized (make sure you categorize them!)\n",
    "    directory=src_path_train,\n",
    "    target_size=(300, 300),\n",
    "    color_mode=\"rgb\", #color images used here, but can also use grey scale\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    subset='training',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "#The thesis of this notebook was to see if a knot was tied properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "extraordinary-workstation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras_preprocessing.image.directory_iterator.DirectoryIterator"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_generator) # memory address of where this will be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "perfect-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "#below is the network used; focus on structure here\n",
    "#The way CNN works, series of convolutional steps that created layers that work as a filter\n",
    "model = Sequential()\n",
    "model.add(Conv2D(16,kernel_size=(5,5),activation='relu',input_shape=(300, 300, 3))) #input shape is a 300 x 300 x 3 pixels x 16 images which is HUGE\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "#This helps to make the size of data more tenable; it finds the largest values (differences), to make filters smaller to manage them\n",
    "# we might be losing resolution when pool\n",
    "#CONVOLUTION MAKES FILTERS, POOLING MAKES THEM SMALLER\n",
    "model.add(Conv2D(16,kernel_size=(5,5),activation='relu')) # 5 x 5 kernel\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(16,kernel_size=(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(16,kernel_size=(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(100, activation='relu')) #push flattened layers into dense NN\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "          \n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=['accuracy']) #classified info comes out, ie what kind of knot is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "covered-trick",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 5 steps\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 12s 2s/step - loss: 1.1070 - accuracy: 0.3750\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.1026 - accuracy: 0.2000\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.1048 - accuracy: 0.3500\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0991 - accuracy: 0.3750\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0982 - accuracy: 0.3750\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0981 - accuracy: 0.3500\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0984 - accuracy: 0.3750\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0984 - accuracy: 0.3500\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0967 - accuracy: 0.3750\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0983 - accuracy: 0.3500\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.1015 - accuracy: 0.3500\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0984 - accuracy: 0.3500\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0986 - accuracy: 0.3500\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0996 - accuracy: 0.3500\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0972 - accuracy: 0.3500\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0973 - accuracy: 0.3500\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0990 - accuracy: 0.3500\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0974 - accuracy: 0.3500\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0985 - accuracy: 0.3500\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0980 - accuracy: 0.3500\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0972 - accuracy: 0.3500\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0955 - accuracy: 0.3500\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0946 - accuracy: 0.3500\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0966 - accuracy: 0.3500\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0949 - accuracy: 0.3500\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0926 - accuracy: 0.3500\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.1001 - accuracy: 0.3500\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0987 - accuracy: 0.3500\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0998 - accuracy: 0.3500\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.1046 - accuracy: 0.3500\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.1012 - accuracy: 0.3500\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.1003 - accuracy: 0.3750\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0975 - accuracy: 0.3750\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0981 - accuracy: 0.3500\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0982 - accuracy: 0.3750\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.1032 - accuracy: 0.2750\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0953 - accuracy: 0.4000\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0961 - accuracy: 0.3750\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0988 - accuracy: 0.3500\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.1030 - accuracy: 0.3000\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0977 - accuracy: 0.4250\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0980 - accuracy: 0.3250\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0986 - accuracy: 0.3500\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0987 - accuracy: 0.3500\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0990 - accuracy: 0.3500\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0985 - accuracy: 0.3500\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0984 - accuracy: 0.3500\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0986 - accuracy: 0.3500\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0979 - accuracy: 0.3500\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 11s 2s/step - loss: 1.0972 - accuracy: 0.3500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd3ff87dda0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch = train_generator.n//train_generator.batch_size, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-exhaust",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
