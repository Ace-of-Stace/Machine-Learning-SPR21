{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lecture: Support Vector Machines** (Iris Data Set)\n",
    "Support-vector machines (SVMs) are supervised learning models used for classification and regression analysis.  \n",
    " <p style=\"text-align: center;\"> <img src= SVM.jpg width=900 alt='[img: SVM]'/>  </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **The Question: Is the flower that we have measured a Iris Setosa?**  \n",
    "For this we will use SVM to make a classifer that will train on a 100 datum training set and test on the remaining flowers.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Numpy\n",
    "import pandas as pd # Pandas is a great tool for working with and displaying data.\n",
    "import matplotlib.pyplot as plt # For plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Reading in the Data** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_FRAME = pd.read_csv('/home/adam_forland/Data/biology/Iris.csv') ## Load data\n",
    "print(RAW_DATA_FRAME.keys())\n",
    "RAW_DATA_FRAME = RAW_DATA_FRAME[[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\",\"PetalWidthCm\", \"Species\" ]]\n",
    "pd.unique(RAW_DATA_FRAME[\"Species\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Colnames = ['SL', 'SW', 'PL', 'PW', 'Flower']\n",
    "RAW_DATA_FRAME.columns = Colnames \n",
    "\n",
    "# Make an index for different binary categories .\n",
    "mapping = {\"Iris-setosa\": -1, \"Iris-versicolor\": 1, \"Iris-virginica\" : 1}\n",
    "\n",
    "# Map Setosa to -1 and the two other flowers to 1.\n",
    "RAW_DATA_FRAME[\"Flower\"] = [mapping[item] for item in RAW_DATA_FRAME[\"Flower\"]]\n",
    "\n",
    "RAW_DATA_FRAME.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Working to make training and testing data** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next section of code allows us to plot different features with respect to each other. For this method of machine learning we are looking for a linear correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# This is the set that controls what features we are using. (As well as the target)\n",
    "ALL_DATA_LABELS = [\"PW\", \"PL\",  \"Flower\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_DATA_SHUFFLED = RAW_DATA_FRAME[ALL_DATA_LABELS].sample(frac=1) # Shuffle the data frame\n",
    "TRAINING = ALL_DATA_SHUFFLED[:100] # Build the training.\n",
    "TESTING = ALL_DATA_SHUFFLED[100:] # Build the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING.head() # Print a part of the testing set to see that it is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the training sets. One for the features and one for the targets.\n",
    "X_TRAIN = np.array(TRAINING.drop(\"Flower\",  axis=1))\n",
    "Y_TRAIN = np.array(TRAINING[\"Flower\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the testing sets. One for the features and one for the targets.\n",
    "X_TEST = np.array(TESTING.drop(\"Flower\",  axis=1))\n",
    "Y_TEST = np.array(TESTING[\"Flower\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Statistical Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data. \n",
    "plt.scatter(TRAINING[ALL_DATA_LABELS[0]], TRAINING[ALL_DATA_LABELS[1]],c = TRAINING[\"Flower\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Support Vector Machines**  \n",
    "**The Math**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING.head() # Look at a piece of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47\n",
    "\n",
    "We need a Cost function and its gradients. The cost function that we will use for SVM is the hinge loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$HL(w_1,w_2,w_3, ... ,w_m) = min_w \\lambda |\\mathbf{w}|^2 + \\sum_{i = 1}^{N} \\textrm{max}(0,1-  y_i \\langle \\mathbf{x}_i, \\mathbf{w} \\rangle) $$  \n",
    "where $$ y_i = \\begin{cases} \n",
    "      -1 & y_i \\ \\ \\textbf{Iris Setosa} \\\\\n",
    "      1 & y_i \\ \\ \\textbf{Not Iris Setosa }\n",
    "   \\end{cases} $$\n",
    "and the \"gradients\" will be,  \n",
    "$$ \\frac{\\partial}{\\partial w_k} \\textrm{max}(0, y_k \\langle \\mathbf{x}_k, \\mathbf{w} \\rangle) = \\begin{cases} \n",
    "      0 & \\ \\ \\textrm{if}, \\ y_k \\langle \\mathbf{x}_k, \\mathbf{w} \\rangle < 0 \\\\\n",
    "      -y_kx_k & \\ \\ \\textrm{else}\n",
    "   \\end{cases} $$\n",
    "\n",
    "and,\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial w_k} min_w \\lambda |\\mathbf{w}|^2 = 2 \\lambda w_k $$  \n",
    "\n",
    "So, the gradients will be different depending on the sign of $\\displaystyle  y_k \\langle \\mathbf{x}_k, \\mathbf{w} \\rangle$.\n",
    "\n",
    "If $\\displaystyle  y_k \\langle \\mathbf{x}_k, \\mathbf{w} \\rangle > 0$ the predicted value was the same sign as the target, and we will update the weights with,\n",
    "$$w_k = w_k - \\alpha(2 \\lambda w_k) $$  \n",
    "If  $\\displaystyle  y_k \\langle \\mathbf{x}_k, \\mathbf{w} \\rangle < 0$ the predicted value was a different sign than the target, and we will update the weights with,  \n",
    "$$w_k = w_k - \\alpha(2 \\lambda w_k - y_kx_k),$$\n",
    "with $\\alpha$ in each case to set our step size (learning rate).\n",
    "\n",
    "### **Bit of a change in structure**\n",
    "\n",
    "The above is a deeper story of Hinge Loss and the error surface. I am going to modify it a bit to better showcase what is happening. \n",
    "\n",
    "For the code below, I am going to update the weights as follows,\n",
    "\n",
    "If $\\displaystyle  y_k \\langle \\mathbf{x}_k, \\mathbf{w} \\rangle > 0$ the predicted value was the same sign as the target, and we will update the weights with,\n",
    "$$w_k = w_k  $$  \n",
    "If  $\\displaystyle  y_k \\langle \\mathbf{x}_k, \\mathbf{w} \\rangle < 0$ the predicted value was a different sign than the target, and we will update the weights with,  \n",
    "$$w_k = w_k + \\alpha y_kx_k,$$\n",
    "with $\\alpha$ in each case to set our step size (learning rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def supportvectormachine(x, Y, learning_Rate, epochs):\n",
    "    archive = []\n",
    "    x =  np.append(x, np.array([np.ones(len(x))]).T, axis=1)\n",
    "    weights = np.random.rand(len(x.T)) # One weight for each column of the feature set. (Another way to say it is a weight for each feature.)\n",
    "    \n",
    "    # Loop over all the data\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        # Create an arcive so that we can plot many different SVM lines.\n",
    "        archive.append([i,weights[0], weights[1]])\n",
    "         \n",
    "        # Predict over all the x feature training data.\n",
    "        y_pred = np.dot(x, weights)\n",
    "        \n",
    "        # Get the value of the true answer times the predicted value. (This is y * <x,w> above)\n",
    "        value = Y * y_pred\n",
    "        \n",
    "        # Set a location. Since we will be working through all the data one at a time, this will keep our position.\n",
    "        location = 0\n",
    "        \n",
    "        # Loop over all the data in the training set. Note in this loop t is set to an element of the array, value.\n",
    "        for t in value:\n",
    "            \n",
    "            # The above sudogradient requires us to check the sign of one part of the gradient. So this gate will do that for us. \n",
    "            \n",
    "            # Correct guess weight update.\n",
    "            if t > 0:\n",
    "                weights = weights  # - learning_Rate * (2 * 1 / epochs * weights )\n",
    "            \n",
    "            # Incorrect guess weight update.\n",
    "            else:\n",
    "                weights  = weights + learning_Rate * ( Y[location] * x[location] ) # - 2 * 1 / epochs * weights)\n",
    "            \n",
    "            # Move one location in the data set (we will need to do this for each epoch)\n",
    "            location += 1\n",
    "    \n",
    "    # Return the weights, the list of intercepts and the list of slopes.\n",
    "    return [weights, archive]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training the Network** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 140000 # Number of epochs.\n",
    "LEARNING_RATE = 10 # Learning rate.\n",
    "\n",
    "w, A = supportvectormachine(X_TRAIN, Y_TRAIN, LEARNING_RATE, EPOCHS) # Run the SVM definition. (Train the model)\n",
    "print(w) # Print our trained weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Development check that the numbers were in the correct range.\n",
    "for i in range(5):\n",
    "    check = np.dot(X_TEST[i].T,  w[:-1]) + w[-1]  # This is how we apply the trained model weights.\n",
    "    print(check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Assessment of the Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "V = [] # Empty list to collect the predictions.\n",
    "\n",
    "x = np.array(X_TRAIN) # Assign the testing data.\n",
    "y = np.array(Y_TRAIN) # Assign the testing targets.\n",
    "\n",
    "# Loop over all the testing features to get the classification probabilities.\n",
    "for i in range(len(x)):\n",
    "    V.append(np.dot(w[:-1], x[i]) + w[-1])\n",
    "\n",
    "new_y_pred = [] # Empty list for the predictions.\n",
    "\n",
    "# If the predicetion value is greater then 1, give it a value of 1 and if it is less than on give it a value of -1.\n",
    "for val in V:\n",
    "    if(val > 0):\n",
    "        new_y_pred.append(1)\n",
    "    else:\n",
    "        new_y_pred.append(-1)    \n",
    "\n",
    "# Print the findings. (I use sklearn to help with counting here.)\n",
    "print(\"Total number correct: \", accuracy_score(y,new_y_pred, normalize = False))\n",
    "print(\"Total number in the testing data: \", len(Y_TRAIN))\n",
    "print(\"Accuracy Score\", accuracy_score(y,new_y_pred, normalize = False) / len(Y_TRAIN) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This min/max stuff is here to set the window size dynamically for different data sets. \n",
    "min0 = RAW_DATA_FRAME[[ALL_DATA_LABELS[0]]].min()\n",
    "min1 = RAW_DATA_FRAME[[ALL_DATA_LABELS[1]]].min()\n",
    "m = min(min0[0], min1[0])\n",
    "\n",
    "max0 = RAW_DATA_FRAME[[ALL_DATA_LABELS[0]]].max()\n",
    "max1 = RAW_DATA_FRAME[[ALL_DATA_LABELS[1]]].max()\n",
    "n = min(max0[0], max1[0])\n",
    "\n",
    "# Set the \"continuous\" domain of values for graphing the line.\n",
    "t1 = np.arange(m, n,.01)\n",
    "\n",
    "# Set the size of the figure.\n",
    "fig = plt.subplots(figsize = (13,13))\n",
    "\n",
    "#Plot many different lines from the archive. \n",
    "#for k in range(EPOCHS):\n",
    "#    if k % 1000 == 0 and k != 0:\n",
    "#        plt.plot(t1, t1 * - A[k][1] / A[k][0], alpha=0.5)\n",
    "        \n",
    "# Plot the data.\n",
    "plt.scatter(RAW_DATA_FRAME[ALL_DATA_LABELS[0]], RAW_DATA_FRAME[ALL_DATA_LABELS[1]], c = RAW_DATA_FRAME[\"Flower\"] , s = 10)\n",
    "plt.plot(t1, t1 *  (- w[0] / w[1]) + (-w[-1]) / w[1], c = 'r' ) # Plot the final line.\n",
    "plt.xlim(0, 7)\n",
    "plt.ylim(0, 7)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.show() # Show the entire plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sklearn implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(gamma='scale')\n",
    "clf.fit(X_TRAIN, Y_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = []\n",
    "\n",
    "for i in range(len(x)):\n",
    "    W.append(clf.predict([X_TEST[i]]))\n",
    "\n",
    "# Print the findings. (I use sklearn to help with counting here.)\n",
    "print(\"Total number correct: \", accuracy_score(y,W, normalize = False))\n",
    "print(\"Total number in the testing data: \", len(Y_TEST))\n",
    "\n",
    "print(\"Accuracy Score\", accuracy_score(y, W, normalize = False) / len(Y_TEST) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
