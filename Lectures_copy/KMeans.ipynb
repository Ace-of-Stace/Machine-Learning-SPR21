{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lecture: K - Means Clustering** \n",
    "The search for the clusters!\n",
    "\n",
    "_Problem:_ You are given a set of data that is unlabeled, but you believe that it can be sorted into groups. You believe that objects that are close together are related. You also know that the data in the given data set is measurements in `cm` of four different parts of some flowers from a field. \n",
    "The following notebook will give us some of the tools to try and determine how the data should be clustered together.  \n",
    "\n",
    "Modules:\n",
    "- Numpy\n",
    "- Pandas\n",
    "- Matplotlib\n",
    "\n",
    "Variables:\n",
    "- `l = 0`  Set a counter for the number of clusters. \n",
    "- `F = 2`  Number of features.\n",
    "- `K` Current number of clusters. This will loop from 1 to 10.\n",
    "- `N = 3` Number of clusters for plotting (late notebook).\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Read In The Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read in the data as a Dataframe and label each Series with a name. (.sample(frac=1) shuffles the data)\n",
    "all_data = pd.read_csv('Data/biology/Garden_Data.csv', names = [\"Feature_1\",\"Feature_2\",\"Feature_3\",\"Feature_4\"], delimiter=',').sample(frac=1)\n",
    "# Make sure to \"point at the right place\" by putting the correct directory path in. \n",
    "print(all_data.head()) # Print some of the data. (head is the start of the data)\n",
    "\n",
    "# Make a Series for two of the features for visulization.  \n",
    "F1 = all_data[\"Feature_1\"]\n",
    "F2 = all_data[\"Feature_2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data with respect to the first two features.\n",
    "plt.scatter(F1, F2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This definition will make the starting centroids for each of cluster counts.\n",
    "def gen_centroids(k,f):\n",
    "    return np.array(all_data.sample(frac=1).iloc[:k,:f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## **Code Block Breakdown**\n",
    "---\n",
    "In this first bit will will establish some of the variables that we will need, as well as some lists that we will fill as the loops run.\n",
    "```Python\n",
    "l = 0 # Set a counter for the number of clusters. \n",
    "F = 2 # Number of features.\n",
    "variance = [] # An empty list to collect the variance. \n",
    "Centroids_Per_Cluster = [] # An empty list to collect the centroids for each cluster setting.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This is the start of the `for` loop that will check cluster sizes. This will run over all the possible amounts of clusters. (really it is 1 through 10). Since we are not yet sure how many classes of objects we will have, letting it check a few will help use to get a better idea. \n",
    "\n",
    "This section of code is also where we will call the above definition, to generate the starting centroids. \n",
    "\n",
    "``` Python\n",
    "# Loop over all the possible number of clusters. This is hard coded to run from 1 to 10.  \n",
    "for K in range(1,10):\n",
    "\n",
    "    centroids = gen_centroids(K,F) # Get the starting set of centroids. \n",
    "\n",
    "    features = np.array(all_data.iloc[:,:F]) # Features from all the data. We can 1 through 4. (2 for 2D scatter plot)  \n",
    "\n",
    "    convergence_check = 1 # Dummy variable to start the while loop. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This `while` loop is the main loop of the algorithm. We want this loop to run until the centroids stop moving at each check. The `convergence_check` is the distance between two consecutive iterations of centroids. Once this distance is close, we can say that the points have converged and move onto the next cluster size.\n",
    "\n",
    "```Python\n",
    "    # Main convergence loop. This will run until two consecutive iterations of centroids are close toegther. \n",
    "    while convergence_check > 0:\n",
    "        \n",
    "        cluster_index = [] # Empty list to collect the cluster index value for each of the points. \n",
    "        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This is where the main mathematics of the algorithm is. This `for` loop checks the distance that each of the data points is from the different clusters. It then selects the closest cluster, and appends the `cluster_index` for that cluster. The distance is measured using,\n",
    "\n",
    "$$ \\sqrt{\\sum{(\\mathbf{\\bar{x}}_i - \\mathbf{x}}) ^2} $$\n",
    "\n",
    "for where each $\\mathbf{\\bar{x}}_i$ is one of the centroids and each $\\mathbf{x}$ is a data point in of our data.\n",
    "\n",
    "```Python\n",
    "\n",
    "        # This will loop over all the data, checking the Euclidean distance to the other centroids.\n",
    "        # This loop also finds the closest centroid for a data point, and gives it that cluster index.\n",
    "        for d in features:\n",
    "            current_centroids = centroids # Set the current centroids to the last ones. \n",
    "            \n",
    "            D = []  # List for the data features for each datum.\n",
    "            for s in range(K): # Loop over the number of clusters. \n",
    "                D.append(d) # Make a duplicate of the features for each datum up to the current number of centroids.\n",
    "\n",
    "            # Measure the distance between each centroid and the current datum.\n",
    "            distance = np.sqrt(np.sum(np.power(centroids - D, 2), axis = 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---         \n",
    "This line of code allow us to find the closest centroid for each data values. We will label each data value but the centroid it is closest to. \n",
    "\n",
    "`np.argmin()` finds the minimum value and returns the index of that value. Since we will have K <-- (number of centroids) distances, we need to determine the minimum one. \n",
    "\n",
    "```Python\n",
    "            # Find the shortest distance. np.argmin will give us the index of this shortest distance.\n",
    "            cluster_index.append(np.argmin(distance))\n",
    "\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will zip the cluster number together with the features for the current datum. Like a zipper, this connects to lists together element by element.\n",
    "The type for `zip` is not a list so I follow with it `list` and then `np.array`. \n",
    "\n",
    "```Python\n",
    "        \n",
    "        # Collect all the datum together with there current cluster index.\n",
    "        clustered = np.array(list(zip(cluster_index, features)), dtype=object)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This next collection of loops is going to calculate the new centroids for each of the newly formed clusters. To compute this we will add all the features for a given cluster together component wise and then divide by how many there are in the cluster. This average will be the new centroids for for each of the newly formed clusters.  \n",
    "\n",
    "```Python\n",
    "        centroids = [] # List for the new centroids.\n",
    "        \n",
    "        # This loop will compute the new centroids for the new way the data is clustered.\n",
    "        for num in range(K):\n",
    "            a = 0 # Reset a counter for each of the clusters. \n",
    "            A = np.zeros(F) # Make an array of zeros of the correct size for the number of features being considered. \n",
    "\n",
    "            # Finding the centroid for each of the new clusters.\n",
    "            for e in clustered:\n",
    "                if e[0] == num: # Check for the current considered cluster index.\n",
    "                    A = A + e[1] # Add the data up (one at a time through this loop)\n",
    "                    a += 1 # Count how many datum are in the current considered cluster.\n",
    "\n",
    "            centroids.append(A / a) # Divide by the amount of data for each cluster.\n",
    "\n",
    "        # Save the new centroids.\n",
    "        centroids = np.array(centroids)\n",
    "        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This next piece of code measures the distance that the last collection of centroids is from the new ones. As we repeat this process over and over, these centroids from two consecutive cluster formulations get closer together. Once the max distance between two consecutive cluster formulation centroids is \"zero\" (as far as the computer can tell) the `while` loop will end. (Information will be collected and computed and then the loop ends)\n",
    "\n",
    "```Python        \n",
    "        # LEARNING! Check the distance between the old centroids and the new ones.\n",
    "        # If this is \"zero\". If the old centroids and the new are the same, and we are done.\n",
    "        convergence_check = np.max(np.sqrt(np.sum(np.power(centroids - current_centroids, 2), axis = 1)))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The next few pieces of code will collect the centroids for each of the formed clusters as well as compute a measure of variance for each formed collection of clusters. We can use the change in this variance to determine the \"correct\" amount of clusters. \n",
    "\n",
    "$$\\sum\\sum(\\mathbf{\\bar{x}}_i - \\mathbf{x})^2 $$\n",
    "\n",
    "\n",
    "```Python\n",
    "    l += 1 # Increase the number of clusters counted.\n",
    "    Centroids_Per_Cluster.append(centroids) # Collect the centroids for each cluster.\n",
    "\n",
    "    # This final set of loops will measure a variance between centroids and the data in each cluster with the other ones.\n",
    "    # We use this to help determine how many clusters we should expect. We use the same measure of distance\n",
    "    # that we used above.\n",
    "    for num in range(K):\n",
    "        Sum = 0\n",
    "        for e in clustered:\n",
    "            if e[0] == num:\n",
    "                Pow = np.sum(np.power(centroids[num] - e[1], 2))\n",
    "                Sum = Sum + Pow\n",
    "           \n",
    "    # Collect this measure of variance and the current cluster number.\n",
    "    variance.append([l, Sum]) \n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The Code Block**\n",
    "\n",
    "This next cell runs all the code that we talked about above. The output of this cell is the limit-centroids for each of the clusters as well as the \"cluster number\" which is the number of clusters that the data has been formed into. It also computes a form of variance to help identify how many clusters we should expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 0 # Set a counter for the number of clusters. \n",
    "F = 2 # Number of features.\n",
    "variance = [] # Set an empty to calcluate the variance. \n",
    "Centroids_Per_Cluster = []\n",
    "\n",
    "\n",
    "# Loop over all the possible number of clusters. This is hard coded to run from 1 to 10.  \n",
    "for K in range(1,10):\n",
    "\n",
    "    centroids = gen_centroids(K,F) # Get the starting set of centroids. \n",
    "\n",
    "    features = np.array(all_data.iloc[:,:F]) # Features from all the data. We can 1 through 4. (2 for 2D scatter plot)\n",
    "\n",
    "    convergence_check = 1 # Dummy variable to start the loop. \n",
    "\n",
    "    \n",
    "    # Main convergence loop. This will run until two consecutive iterations of centroids are close toegther. \n",
    "    while convergence_check > 0:\n",
    "        \n",
    "        cluster_index = [] # Empty list to collect the cluster index value for each of the points. \n",
    "\n",
    "        # This will loop over all the data, checking the Euclidean distance to the other centroids.\n",
    "        # This loop also finds the closest centroid for a data point, and gives it that cluster index.\n",
    "        for d in features:\n",
    "            current_centroids = centroids # Set the current centroids to the last ones. \n",
    "            \n",
    "            D = [] # List for the data features for each datum.\n",
    "            for s in range(K): # Loop over the number of clusters. \n",
    "                D.append(d) # Make a duplicate of the features for each datum up to the current number of centroids.\n",
    "            \n",
    "            # Measure the distance between each centriod and the current datum.\n",
    "            distance = np.sqrt(np.sum(np.power(centroids - D, 2), axis = 1)) \n",
    "            \n",
    "            # Find the shortest distance. np.argmin will give us the index of this shortest distance. \n",
    "            cluster_index.append(np.argmin(distance))\n",
    "\n",
    "\n",
    "        # Collect all the datum together with there current cluster index.\n",
    "        clustered = np.array(list(zip(cluster_index, features)), dtype=object)\n",
    "        \n",
    "        centroids = [] # List for the new centroids.\n",
    "        \n",
    "        # This loop will compute the new centroids for the new way the data is clustered.\n",
    "        for num in range(K):\n",
    "            a = 0 # Reset a counter for each of the clusters. \n",
    "            A = np.zeros(F) # Make an array of zeros of the correct size for the number of features being considered. \n",
    "\n",
    "            # Finding the centroid for each of the new clusters.\n",
    "            for e in clustered:\n",
    "                if e[0] == num: # Check for the current considered cluster index.\n",
    "                    A = A + e[1] # Add the data up (one at a time through this loop)\n",
    "                    a += 1 # Count how many datum are in the current considered cluster.\n",
    "\n",
    "            centroids.append(A / a) # Divide by the amount of data for each cluster.\n",
    "\n",
    "        # Save the new centroids.\n",
    "        centroids = np.array(centroids)\n",
    "\n",
    "        # LEARNING! Check the distance between the old centroids and the new ones.\n",
    "        # If this is \"zero\". If the old centroids and the new are the same, we are done.\n",
    "        convergence_check = np.max(np.sqrt(np.sum(np.power(centroids - current_centroids, 2), axis = 1)))\n",
    "\n",
    "\n",
    "    l += 1 # Increase the number of clusters counted.\n",
    "    Centroids_Per_Cluster.append(centroids) # Collect the centroids for each cluster.\n",
    "\n",
    "    # This final set of loops will measure a variance between centroids and the data in each cluster with the other ones.\n",
    "    # We use this to help determine how many clusters we should expect. We use the same measure of distance\n",
    "    # that we used above.\n",
    "    Sum = 0\n",
    "    for num in range(K):\n",
    "        for e in clustered:\n",
    "            if e[0] == num:\n",
    "                Pow = np.sum(np.power(centroids[num] - e[1], 2))\n",
    "                Sum = Sum + Pow\n",
    "           \n",
    "    # Collect this measure of variance and the current cluster number.\n",
    "    variance.append([l, Sum])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## **Determine the Number of clusters**\n",
    "\n",
    "In the next visulization we will use the [Elbow Method](https://en.wikipedia.org/wiki/Elbow_method_(clustering)) to determine the \"correct\" number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These lines of code will plot the variance with respect to the cluster number.\n",
    "variance = np.array(variance)\n",
    "plt.scatter(variance[:,0], variance[:,1]) \n",
    "plt.plot(variance[:,0], variance[:,1]) \n",
    "plt.xlabel(\"Cluster Number\")\n",
    "plt.ylabel(\"Variance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By changing the number in the next line of code, we can see the centroids for the different cluster amounts.\n",
    "print(Centroids_Per_Cluster[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two cells will allow us to see the clusters for each of the cluster numbers. Change the N and rerun the cells to see the found clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "\n",
    "features = np.array(all_data.iloc[:,:F])\n",
    "cluster_index = []\n",
    "\n",
    "for d in features:\n",
    "    centroids = Centroids_Per_Cluster[N - 1]\n",
    "\n",
    "    D = [] \n",
    "    for s in range(N):\n",
    "        D.append(d)\n",
    "\n",
    "    distance = np.sqrt(np.sum(np.power(centroids - D, 2), axis = 1))\n",
    "\n",
    "    cluster_index.append(np.argmin(distance))\n",
    "\n",
    "\n",
    "clustered = np.array(list(zip(cluster_index, features)), dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#info = pd.factorize(all_data.iloc[:,4])\n",
    "plt.scatter(F1, F2, c = clustered[:,0])\n",
    "plt.scatter(Centroids_Per_Cluster[N - 1][:,0], Centroids_Per_Cluster[N - 1][:,1], c = \"red\", marker='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
