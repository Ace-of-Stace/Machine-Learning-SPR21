{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lecture: Logistic Regression using Gradient Descent** (Liver Data Set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/uciml/indian-liver-patient-records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Numpy\n",
    "import pandas as pd # Pandas is a great tool for working with and displaying data.\n",
    "import matplotlib.pyplot as plt # For plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>TOTAL_BIL</th>\n",
       "      <th>DIR_BIL</th>\n",
       "      <th>ALK_PHO</th>\n",
       "      <th>ALA_AMINO</th>\n",
       "      <th>ASP_AMI</th>\n",
       "      <th>TOTAL_PROT</th>\n",
       "      <th>ALBU</th>\n",
       "      <th>ALBU_GLO</th>\n",
       "      <th>DATA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>187.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>5.5</td>\n",
       "      <td>699.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.3</td>\n",
       "      <td>4.1</td>\n",
       "      <td>490.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>182.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>72.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    AGE  GENDER  TOTAL_BIL  DIR_BIL  ALK_PHO  ALA_AMINO  ASP_AMI  TOTAL_PROT  \\\n",
       "1  65.0     1.0        0.7      0.1    187.0       16.0     18.0         6.8   \n",
       "2  62.0     2.0       10.9      5.5    699.0       64.0    100.0         7.5   \n",
       "3  62.0     2.0        7.3      4.1    490.0       60.0     68.0         7.0   \n",
       "4  58.0     2.0        1.0      0.4    182.0       14.0     20.0         6.8   \n",
       "5  72.0     2.0        3.9      2.0    195.0       27.0     59.0         7.3   \n",
       "\n",
       "   ALBU  ALBU_GLO  DATA  \n",
       "1   3.3      0.90   1.0  \n",
       "2   3.2      0.74   1.0  \n",
       "3   3.3      0.89   1.0  \n",
       "4   3.4      1.00   1.0  \n",
       "5   2.4      0.40   1.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Data Prep Step for liver set.\n",
    "#RAW_DATA = np.genfromtxt(\"~/Data/healthcare/liver\", delimiter=',')\n",
    "RAW_DATA_FRAME = pd.read_csv(\"~/Data/healthcare/liver/indian_liver_patient.csv\", header = None)\n",
    "RAW_DATA_FRAME = RAW_DATA_FRAME.iloc[1:] # Remove the 0 row.\n",
    "\n",
    "# Give the columns keywords.\n",
    "Colnames = ['AGE', 'GENDER', 'TOTAL_BIL', 'DIR_BIL', 'ALK_PHO', 'ALA_AMINO', 'ASP_AMI', 'TOTAL_PROT', 'ALBU', 'ALBU_GLO', 'DATA']\n",
    "RAW_DATA_FRAME.columns = Colnames \n",
    "\n",
    "# Make a numeric index for the included genders.\n",
    "mapping = {'Female': 1, 'Male': 2}\n",
    "\n",
    "# Map Female to 1 and Male to 2.\n",
    "RAW_DATA_FRAME[\"GENDER\"] = [mapping[item] for item in RAW_DATA_FRAME[\"GENDER\"]]\n",
    "\n",
    "# Make all the entries in the dataframe into floats. (They were read in as strings.)\n",
    "cols = RAW_DATA_FRAME.columns\n",
    "for col in cols:\n",
    "    RAW_DATA_FRAME[col] = RAW_DATA_FRAME[col].astype(float)\n",
    "\n",
    "# Print a small part of the data.\n",
    "RAW_DATA_FRAME.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next block is for when we switch the the Iris set.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SL</th>\n",
       "      <th>SW</th>\n",
       "      <th>PL</th>\n",
       "      <th>PW</th>\n",
       "      <th>Flower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SL   SW   PL   PW  Flower\n",
       "0  4.9  3.0  1.4  0.2       0\n",
       "1  4.7  3.2  1.3  0.2       0\n",
       "2  4.6  3.1  1.5  0.2       0\n",
       "3  5.0  3.6  1.4  0.2       0\n",
       "4  5.4  3.9  1.7  0.4       0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' # Reimagining of iris data set\n",
    "RAW_DATA_FRAME = pd.read_csv('/home/MLDir/Data/iris_KNN.csv') ## Load data\n",
    "RAW_DATA_FRAME = RAW_DATA_FRAME.iloc[:,1:]\n",
    "\n",
    "Colnames = ['SL', 'SW', 'PL', 'PW', 'Flower']\n",
    "RAW_DATA_FRAME.columns = Colnames \n",
    "\n",
    "# Make an index for the included genders.\n",
    "mapping = {0: 0, 1: 1, 2 : 1}\n",
    "\n",
    "# Map Female to 1 and Male to 2.\n",
    "RAW_DATA_FRAME[\"Flower\"] = [mapping[item] for item in RAW_DATA_FRAME[\"Flower\"]]\n",
    "\n",
    "RAW_DATA_FRAME.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Working to make training and testing data** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next section of code allows us to plot different features with respect to each other. For this method of machine learning we are looking for a linear correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_DATA_LABELS = [\"ALBU\", \"TOTAL_BIL\", \"DIR_BIL\", \"ALA_AMINO\",\"AGE\",\"ALK_PHO\",\"ASP_AMI\", \"DATA\"] #Toggles between iris and liver data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALL_DATA_LABELS = [\"SL\",\"SW\",\"PL\",\"PW\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_DATA_SHUFFLED = RAW_DATA_FRAME[ALL_DATA_LABELS].sample(frac=1) # Shuffle the data frame\n",
    "TRAINING = ALL_DATA_SHUFFLED[:10] # Build the training.\n",
    "TESTING = ALL_DATA_SHUFFLED[10:] # Build the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ALBU</th>\n",
       "      <th>TOTAL_BIL</th>\n",
       "      <th>DIR_BIL</th>\n",
       "      <th>ALA_AMINO</th>\n",
       "      <th>AGE</th>\n",
       "      <th>ALK_PHO</th>\n",
       "      <th>ASP_AMI</th>\n",
       "      <th>DATA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>91.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>3.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>48.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>3.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>23.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>21.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ALBU  TOTAL_BIL  DIR_BIL  ALA_AMINO   AGE  ALK_PHO  ASP_AMI  DATA\n",
       "16    2.3        0.6      0.1       91.0  25.0    183.0     53.0   2.0\n",
       "174   3.7        0.6      0.1       48.0  31.0    175.0     34.0   1.0\n",
       "557   3.1        0.7      0.1       25.0  51.0    180.0     27.0   1.0\n",
       "391   3.0        0.8      0.2       23.0  72.0    148.0     35.0   1.0\n",
       "51    2.5        0.7      0.2       21.0  45.0    170.0     14.0   1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TESTING.head() # So we need 8 features to analyze and one target we are training for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN =  np.array(TRAINING.drop(\"DATA\",  axis=1)) # Remove the target column\n",
    "Y_TRAIN = abs(np.array(TRAINING[\"DATA\"] - 2)) # Change the targets to 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_TRAIN =  np.array(TRAINING.drop(\"Flower\",  axis=1))\n",
    "#Y_TRAIN = np.array(TRAINING[\"Flower\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TEST = np.array(TESTING.drop(\"DATA\",  axis=1))\n",
    "Y_TEST = abs(np.array(TESTING[\"DATA\"] - 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_TEST = np.array(TESTING.drop(\"Flower\",  axis=1))\n",
    "#Y_TEST = np.array(TESTING[\"Flower\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Logistic Regression with Gradient Descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Goal: Find a line that best fits the data.**\n",
    "\n",
    "---\n",
    "First recall that the equation of the sigmoid is:\n",
    "$$ \\textrm{Sigmoid function:} \\ \\ S(x) = \\frac{1}{1 + e^{-x}} $$  \n",
    "$$ \\textrm{Sigmoid function's Derivative:} \\ \\ S'(x) = S(x)(1 - S(x)) $$  \n",
    "\n",
    "Now, each data value will be written as,\n",
    "$$z = b + w_1 x_1 + w_2 x_2 + ... + w_m x_m = b +  \\mathbf{W}^{T} \\cdot \\mathbf{X}   $$  \n",
    "If we take this value, and evaluate it in the sigmoid, we will get a value between 0 and 1. So,\n",
    "$$S(z) = \\frac{1}{1 + e^{-z}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def S(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "t = np.arange(-10,10, .01)\n",
    "plt.plot(t,S(t))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we are going to use gradient decent to find the best values for $b ,w_1,w_2...w_m$.  \n",
    "There is one more thing that we will need. Since the idea here is to output information for a binary situation, we need to make sure that our target data is 0 or 1. We did this above in the data prep step.\n",
    "$$\\hat{y} = \\begin{cases} \n",
    "      0 & y_i \\ \\ \\textbf{is positive} \\\\\n",
    "      1 & y_i \\ \\ \\textbf{is negative}\n",
    "   \\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function that we want to minimize is:\n",
    "$$ CE(b,w_1,w_2...w_m) = -\\frac{1}{N}\\sum_{i=1}^{N}\\hat{y}_{i} \\log(S(z)) + (1 - \\hat{y}_{i}) \\log(1 - S(z)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Compute the Gradient:**  \n",
    "The following is the notation that we will need to write down this important function. Since everything that we will be working with from here on out will have multiple feature vectors, we are going to be using gradient as the derivative we will be considering.\n",
    "\n",
    "So, let's compute this object:  \n",
    "$$\\nabla CE(b,w_1,w_2...w_m) = \\left\\langle \\frac{1}{N} \\sum_{i=1}^N (S(z) - \\hat{y}_i) , \\frac{x_1}{N} \\sum_{i=1}^N (S(z) - \\hat{y}_i) , \\frac{x_2}{N} \\sum_{i=1}^N (S(z) - \\hat{y}_i) , ... , \\frac{x_m}{N} \\sum_{i=1}^N (S(z) - \\hat{y}_i) \\right\\rangle $$  \n",
    "\n",
    "So to apply gradient descent we need to evaluate this vector over and over as we update the weights,\n",
    "$$ \\langle b, w_1, w_2, w_3, ... ,w_m \\rangle - \\left\\langle \\frac{1}{N} \\sum_{i=1}^N (S(z) - \\hat{y}_i) , \\frac{x_1}{N} \\sum_{i=1}^N (S(z) - \\hat{y}_i) , \\frac{x_2}{N} \\sum_{i=1}^N (S(z) - \\hat{y}_i) , ... , \\frac{x_m}{N} \\sum_{i=1}^N (S(z) - \\hat{y}_i) \\right\\rangle $$  \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The Code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$S(z) = \\frac{1}{1 + e^{-z}} $$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makegradient(X, Y, weights):\n",
    "    Y = np.array(Y) # Make the target a numpy array.\n",
    "    N  = len(X) # Assign the working length.\n",
    "    \n",
    "    # Make the linear z values.\n",
    "    z = (weights *  X).sum(axis=1)    \n",
    "     \n",
    "    # Apply the sigmoid.\n",
    "    Squshed_Z = sigmoid(z)\n",
    "    \n",
    "    # Make the gradients.\n",
    "    B = np.dot(np.ones((len(Y),1)).T, Squshed_Z - Y) / N\n",
    "    S = np.dot(X.T, Squshed_Z - Y) / N\n",
    "    \n",
    "    return [S , B] # Return the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentLogisticRegression(x, Y, learning_Rate, epochs):\n",
    "        \n",
    "    total_data = len(Y) # This is the N from above.\n",
    "    weights = np.zeros(len(x.T)) # Random weights for our slope and intercept.\n",
    "    B_VAL = np.zeros(1)\n",
    "    \n",
    "    # Loop over all the epochs.\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        # Find the gradients.\n",
    "        grad = makegradient(x, Y, weights)\n",
    "        \n",
    "        # Update the weights.\n",
    "        weights = weights - learning_Rate * grad[0]\n",
    "        print(grad[0])\n",
    "        B_VAL = B_VAL - learning_Rate * grad[1]\n",
    "\n",
    "    \n",
    "    # Return the weights, the list of intercepts and the list of slopes.\n",
    "    return [weights, B_VAL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.02  -0.8   -1.825 -0.635]\n",
      "[ 0.39239432  0.34576858 -0.0869426  -0.06551388]\n",
      "[ 0.23266377  0.26386532 -0.18739738 -0.09664338]\n",
      "[ 0.14968859  0.2197276  -0.23585545 -0.11119166]\n",
      "[ 0.11723774  0.20039871 -0.24991326 -0.11473467]\n",
      "[ 0.10466282  0.19086288 -0.25051079 -0.11398112]\n",
      "[ 0.0984321   0.18448801 -0.24689441 -0.11189492]\n",
      "[ 0.09416844  0.17912824 -0.24205455 -0.1094344 ]\n",
      "[ 0.09057454  0.17415111 -0.23688626 -0.10688721]\n",
      "[ 0.08727041  0.16937583 -0.23166247 -0.1043421 ]\n",
      "[ 0.08414342  0.16475284 -0.22647389 -0.1018295 ]\n",
      "[ 0.08115909  0.16027023 -0.22135626 -0.09936204]\n",
      "[ 0.07830571  0.15592636 -0.2163275  -0.0969464 ]\n",
      "[ 0.07557799  0.15172171 -0.21139879 -0.09458681]\n",
      "[ 0.07297212  0.14765661 -0.20657804 -0.09228621]\n",
      "[ 0.07048445  0.1437306  -0.20187105 -0.09004666]\n",
      "[ 0.06811104  0.13994238 -0.19728205 -0.08786949]\n",
      "[ 0.06584773  0.13628988 -0.19281391 -0.08575544]\n",
      "[ 0.06369013  0.13277043 -0.18846837 -0.08370476]\n",
      "[ 0.06163373  0.12938084 -0.18424616 -0.08171725]\n",
      "[ 0.05967397  0.12611753 -0.18014719 -0.07979238]\n",
      "[ 0.05780629  0.12297666 -0.17617068 -0.07792928]\n",
      "[ 0.05602621  0.11995417 -0.17231521 -0.07612688]\n",
      "[ 0.05432932  0.11704588 -0.16857893 -0.07438389]\n",
      "[ 0.05271137  0.11424756 -0.16495955 -0.07269887]\n",
      "[ 0.05116822  0.11155495 -0.16145449 -0.07107028]\n",
      "[ 0.04969592  0.10896384 -0.15806095 -0.06949648]\n",
      "[ 0.04829069  0.10647005 -0.15477591 -0.06797578]\n",
      "[ 0.04694891  0.1040695  -0.15159626 -0.06650646]\n",
      "[ 0.04566716  0.10175822 -0.14851877 -0.06508678]\n",
      "[ 0.04444218  0.09953235 -0.14554021 -0.06371499]\n",
      "[ 0.04327088  0.09738815 -0.1426573  -0.06238937]\n",
      "[ 0.04215036  0.09532202 -0.13986677 -0.06110822]\n",
      "[ 0.04107787  0.09333051 -0.13716541 -0.05986987]\n",
      "[ 0.04005082  0.0914103  -0.13455003 -0.05867268]\n",
      "[ 0.03906677  0.0895582  -0.13201752 -0.05751506]\n",
      "[ 0.03812341  0.08777118 -0.12956482 -0.05639546]\n",
      "[ 0.03721858  0.08604634 -0.12718897 -0.05531239]\n",
      "[ 0.03635026  0.08438091 -0.12488709 -0.05426441]\n",
      "[ 0.03551652  0.08277225 -0.12265639 -0.05325012]\n",
      "[ 0.03471556  0.08121785 -0.12049416 -0.05226817]\n",
      "[ 0.0339457   0.07971531 -0.1183978  -0.05131729]\n",
      "[ 0.03320534  0.07826238 -0.1163648  -0.05039622]\n",
      "[ 0.03249299  0.07685687 -0.11439274 -0.04950378]\n",
      "[ 0.03180723  0.07549673 -0.11247929 -0.04863883]\n",
      "[ 0.03114675  0.07418002 -0.11062221 -0.04780028]\n",
      "[ 0.03051029  0.07290487 -0.10881936 -0.04698708]\n",
      "[ 0.02989668  0.07166951 -0.10706868 -0.04619823]\n",
      "[ 0.02930481  0.07047227 -0.10536819 -0.04543278]\n",
      "[ 0.02873366  0.06931155 -0.103716   -0.04468981]\n",
      "[ 0.02818222  0.06818584 -0.1021103  -0.04396845]\n",
      "[ 0.02764959  0.06709369 -0.10054935 -0.04326786]\n",
      "[ 0.02713488  0.06603374 -0.09903149 -0.04258724]\n",
      "[ 0.02663727  0.06500467 -0.09755511 -0.04192583]\n",
      "[ 0.02615598  0.06400526 -0.0961187  -0.0412829 ]\n",
      "[ 0.02569029  0.06303432 -0.09472079 -0.04065775]\n",
      "[ 0.02523949  0.06209072 -0.09335999 -0.04004972]\n",
      "[ 0.02480293  0.06117341 -0.09203495 -0.03945817]\n",
      "[ 0.02437999  0.06028136 -0.0907444  -0.03888249]\n",
      "[ 0.02397008  0.0594136  -0.0894871  -0.0383221 ]\n",
      "[ 0.02357265  0.0585692  -0.08826189 -0.03777644]\n",
      "[ 0.02318717  0.0577473  -0.08706763 -0.03724498]\n",
      "[ 0.02281314  0.05694704 -0.08590324 -0.03672722]\n",
      "[ 0.02245009  0.05616763 -0.0847677  -0.03622266]\n",
      "[ 0.02209757  0.05540831 -0.08366002 -0.03573084]\n",
      "[ 0.02175515  0.05466834 -0.08257924 -0.03525131]\n",
      "[ 0.02142243  0.05394703 -0.08152446 -0.03478366]\n",
      "[ 0.02109902  0.05324371 -0.0804948  -0.03432746]\n",
      "[ 0.02078456  0.05255776 -0.07948943 -0.03388234]\n",
      "[ 0.0204787   0.05188855 -0.07850755 -0.0334479 ]\n",
      "[ 0.02018112  0.05123552 -0.07754839 -0.0330238 ]\n",
      "[ 0.01989149  0.0505981  -0.07661122 -0.0326097 ]\n",
      "[ 0.01960952  0.04997577 -0.07569531 -0.03220525]\n",
      "[ 0.01933492  0.04936802 -0.0748     -0.03181015]\n",
      "[ 0.01906742  0.04877435 -0.07392463 -0.03142409]\n",
      "[ 0.01880677  0.04819431 -0.07306857 -0.03104678]\n",
      "[ 0.0185527   0.04762745 -0.07223123 -0.03067794]\n",
      "[ 0.018305    0.04707333 -0.07141202 -0.0303173 ]\n",
      "[ 0.01806343  0.04653156 -0.07061038 -0.02996461]\n",
      "[ 0.01782778  0.04600173 -0.06982579 -0.02961961]\n",
      "[ 0.01759784  0.04548347 -0.06905772 -0.02928208]\n",
      "[ 0.01737341  0.04497641 -0.06830568 -0.02895177]\n",
      "[ 0.01715432  0.04448021 -0.0675692  -0.02862848]\n",
      "[ 0.01694037  0.04399454 -0.06684781 -0.02831198]\n",
      "[ 0.0167314   0.04351906 -0.06614109 -0.02800208]\n",
      "[ 0.01652724  0.04305348 -0.06544859 -0.02769858]\n",
      "[ 0.01632773  0.0425975  -0.06476991 -0.0274013 ]\n",
      "[ 0.01613273  0.04215082 -0.06410465 -0.02711004]\n",
      "[ 0.01594208  0.04171319 -0.06345245 -0.02682464]\n",
      "[ 0.01575565  0.04128432 -0.06281292 -0.02654493]\n",
      "[ 0.01557331  0.04086398 -0.06218572 -0.02627075]\n",
      "[ 0.01539492  0.04045191 -0.0615705  -0.02600194]\n",
      "[ 0.01522037  0.04004788 -0.06096694 -0.02573834]\n",
      "[ 0.01504953  0.03965165 -0.06037472 -0.02547982]\n",
      "[ 0.0148823   0.03926303 -0.05979353 -0.02522624]\n",
      "[ 0.01471856  0.03888178 -0.05922308 -0.02497745]\n",
      "[ 0.01455821  0.03850771 -0.05866307 -0.02473334]\n",
      "[ 0.01440115  0.03814063 -0.05811324 -0.02449376]\n",
      "[ 0.01424729  0.03778034 -0.05757333 -0.02425861]\n",
      "[ 0.01409652  0.03742666 -0.05704306 -0.02402777]\n",
      "The weights are:  [-0.22881311 -0.78780763  1.37080418  0.58789805] with a b value of:  [-0.16951579]\n"
     ]
    }
   ],
   "source": [
    "w, b = gradientDescentLogisticRegression(X_TRAIN, Y_TRAIN, .1, 100) # Call the definition with our data sets.\n",
    "print(\"The weights are: \",  w, \"with a b value of: \", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number correct:  139\n",
      "Total number in the testing data:  139\n",
      "Accuracy Score 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "V = [] # Empty list to collect the predictions.\n",
    "\n",
    "x = np.array(X_TEST) # Assign the testing data.\n",
    "y = np.array(Y_TEST) # Assign the testing targets.\n",
    "\n",
    "# Loop over all the testing features to get the classification probabilities.\n",
    "for i in range(len(x)):\n",
    "    V.append(sigmoid(np.dot(w, x[i]) + b))\n",
    "\n",
    "new_y_pred = [] # Empty list for the predictions.\n",
    "\n",
    "# Set the tolerance to 50%.\n",
    "for val in V:\n",
    "    if(val >= 0.5):\n",
    "        new_y_pred.append(1)\n",
    "    else:\n",
    "        new_y_pred.append(0)    \n",
    "\n",
    "# Print the findings. (I use sklearn to help with counting here.)\n",
    "print(\"Total number correct: \", accuracy_score(y,new_y_pred, normalize = False))\n",
    "print(\"Total number in the testing data: \", len(Y_TEST))\n",
    "\n",
    "print(\"Accuracy Score\", accuracy_score(y,new_y_pred, normalize = False) / len(Y_TEST) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **WARNING SKLEARN IS AWESOME!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create logistic regression object\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(X_TRAIN, Y_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_skit = clf.predict(X_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "new_y_pred_skit = []\n",
    "\n",
    "for val in y_pred_skit:\n",
    "    if(val >= 0.5):\n",
    "        new_y_pred_skit.append(1)\n",
    "    else:\n",
    "        new_y_pred_skit.append(0) \n",
    "\n",
    "print(accuracy_score(y,new_y_pred_skit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
